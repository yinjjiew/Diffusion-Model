{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d399a5-f991-4f78-a724-b16198d240bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "root = '../../data/CIFAR10/'\n",
    "\n",
    "# Load the CIFAR-10\n",
    "train_dataset = torchvision.datasets.CIFAR10(root=root, train=True, transform=transform, download=False)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root=root, train=False, transform=transform, download=False)\n",
    "\n",
    "bs = 100\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff4dbfe-4463-432b-b7e7-7b4366bfd21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "import math\n",
    "from torch.nn import init\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "    \n",
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, T, d_model, dim):\n",
    "        assert d_model % 2 == 0\n",
    "        super().__init__()\n",
    "        emb = torch.arange(0, d_model, step=2) / d_model * math.log(10000)\n",
    "        emb = torch.exp(-emb)\n",
    "        pos = torch.arange(T).float()\n",
    "        emb = pos[:, None] * emb[None, :]\n",
    "        assert list(emb.shape) == [T, d_model // 2]\n",
    "        emb = torch.stack([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
    "        assert list(emb.shape) == [T, d_model // 2, 2]\n",
    "        emb = emb.view(T, d_model)\n",
    "\n",
    "        self.timembedding = nn.Sequential(\n",
    "            nn.Embedding.from_pretrained(emb),\n",
    "            nn.Linear(d_model, dim),\n",
    "            Swish(),\n",
    "            nn.Linear(dim, dim),\n",
    "        )\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                init.xavier_uniform_(module.weight)\n",
    "                init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, t):\n",
    "        emb = self.timembedding(t)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super().__init__()\n",
    "        self.main = nn.Conv2d(in_ch, in_ch, 3, stride=2, padding=1)\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        init.xavier_uniform_(self.main.weight)\n",
    "        init.zeros_(self.main.bias)\n",
    "\n",
    "    def forward(self, x, temb):\n",
    "        x = self.main(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super().__init__()\n",
    "        self.main = nn.Conv2d(in_ch, in_ch, 3, stride=1, padding=1)\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        init.xavier_uniform_(self.main.weight)\n",
    "        init.zeros_(self.main.bias)\n",
    "\n",
    "    def forward(self, x, temb):\n",
    "        _, _, H, W = x.shape\n",
    "        x = F.interpolate(\n",
    "            x, scale_factor=2, mode='nearest')\n",
    "        x = self.main(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttnBlock(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super().__init__()\n",
    "        self.group_norm = nn.GroupNorm(32, in_ch)\n",
    "        self.proj_q = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
    "        self.proj_k = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
    "        self.proj_v = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
    "        self.proj = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        for module in [self.proj_q, self.proj_k, self.proj_v, self.proj]:\n",
    "            init.xavier_uniform_(module.weight)\n",
    "            init.zeros_(module.bias)\n",
    "        init.xavier_uniform_(self.proj.weight, gain=1e-5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        h = self.group_norm(x)\n",
    "        q = self.proj_q(h)\n",
    "        k = self.proj_k(h)\n",
    "        v = self.proj_v(h)\n",
    "\n",
    "        q = q.permute(0, 2, 3, 1).view(B, H * W, C)\n",
    "        k = k.view(B, C, H * W)\n",
    "        w = torch.bmm(q, k) * (int(C) ** (-0.5))\n",
    "        assert list(w.shape) == [B, H * W, H * W]\n",
    "        w = F.softmax(w, dim=-1)\n",
    "\n",
    "        v = v.permute(0, 2, 3, 1).view(B, H * W, C)\n",
    "        h = torch.bmm(w, v)\n",
    "        assert list(h.shape) == [B, H * W, C]\n",
    "        h = h.view(B, H, W, C).permute(0, 3, 1, 2)\n",
    "        h = self.proj(h)\n",
    "\n",
    "        return x + h\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, tdim, dropout, attn=False):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.GroupNorm(32, in_ch),\n",
    "            Swish(),\n",
    "            nn.Conv2d(in_ch, out_ch, 3, stride=1, padding=1),\n",
    "        )\n",
    "        self.temb_proj = nn.Sequential(\n",
    "            Swish(),\n",
    "            nn.Linear(tdim, out_ch),\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.GroupNorm(32, out_ch),\n",
    "            Swish(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1),\n",
    "        )\n",
    "        if in_ch != out_ch:\n",
    "            self.shortcut = nn.Conv2d(in_ch, out_ch, 1, stride=1, padding=0)\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "        if attn:\n",
    "            self.attn = AttnBlock(out_ch)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "                init.xavier_uniform_(module.weight)\n",
    "                init.zeros_(module.bias)\n",
    "        init.xavier_uniform_(self.block2[-1].weight, gain=1e-5)\n",
    "\n",
    "    def forward(self, x, temb):\n",
    "        h = self.block1(x)\n",
    "        h += self.temb_proj(temb)[:, :, None, None]\n",
    "        h = self.block2(h)\n",
    "\n",
    "        h = h + self.shortcut(x)\n",
    "        h = self.attn(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, T, ch, ch_mult, attn, num_res_blocks, dropout):\n",
    "        super().__init__()\n",
    "        assert all([i < len(ch_mult) for i in attn]), 'attn index out of bound'\n",
    "        tdim = ch * 4\n",
    "        self.time_embedding = TimeEmbedding(T, ch, tdim)\n",
    "\n",
    "        self.head = nn.Conv2d(3, ch, kernel_size=3, stride=1, padding=1)\n",
    "        self.downblocks = nn.ModuleList()\n",
    "        chs = [ch]  # record output channel when dowmsample for upsample\n",
    "        now_ch = ch\n",
    "        for i, mult in enumerate(ch_mult):\n",
    "            out_ch = ch * mult\n",
    "            for _ in range(num_res_blocks):\n",
    "                self.downblocks.append(ResBlock(\n",
    "                    in_ch=now_ch, out_ch=out_ch, tdim=tdim,\n",
    "                    dropout=dropout, attn=(i in attn)))\n",
    "                now_ch = out_ch\n",
    "                chs.append(now_ch)\n",
    "            if i != len(ch_mult) - 1:\n",
    "                self.downblocks.append(DownSample(now_ch))\n",
    "                chs.append(now_ch)\n",
    "\n",
    "        self.middleblocks = nn.ModuleList([\n",
    "            ResBlock(now_ch, now_ch, tdim, dropout, attn=True),\n",
    "            ResBlock(now_ch, now_ch, tdim, dropout, attn=False),\n",
    "        ])\n",
    "\n",
    "        self.upblocks = nn.ModuleList()\n",
    "        for i, mult in reversed(list(enumerate(ch_mult))):\n",
    "            out_ch = ch * mult\n",
    "            for _ in range(num_res_blocks + 1):\n",
    "                self.upblocks.append(ResBlock(\n",
    "                    in_ch=chs.pop() + now_ch, out_ch=out_ch, tdim=tdim,\n",
    "                    dropout=dropout, attn=(i in attn)))\n",
    "                now_ch = out_ch\n",
    "            if i != 0:\n",
    "                self.upblocks.append(UpSample(now_ch))\n",
    "        assert len(chs) == 0\n",
    "\n",
    "        self.tail = nn.Sequential(\n",
    "            nn.GroupNorm(32, now_ch),\n",
    "            Swish(),\n",
    "            nn.Conv2d(now_ch, 3, 3, stride=1, padding=1)\n",
    "        )\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        init.xavier_uniform_(self.head.weight)\n",
    "        init.zeros_(self.head.bias)\n",
    "        init.xavier_uniform_(self.tail[-1].weight, gain=1e-5)\n",
    "        init.zeros_(self.tail[-1].bias)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Timestep embedding\n",
    "        temb = self.time_embedding(t)\n",
    "        # Downsampling\n",
    "        h = self.head(x)\n",
    "        hs = [h]\n",
    "        for layer in self.downblocks:\n",
    "            h = layer(h, temb)\n",
    "            hs.append(h)\n",
    "        # Middle\n",
    "        for layer in self.middleblocks:\n",
    "            h = layer(h, temb)\n",
    "        # Upsampling\n",
    "        for layer in self.upblocks:\n",
    "            if isinstance(layer, ResBlock):\n",
    "                h = torch.cat([h, hs.pop()], dim=1)\n",
    "            h = layer(h, temb)\n",
    "        h = self.tail(h)\n",
    "\n",
    "        assert len(hs) == 0\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4448693e-1adb-47f9-927a-9d90fe936f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1000\n",
    "beta_1 = 1e-4\n",
    "beta_T = 0.02\n",
    "n_epochs = 500\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd77e50f-931c-4a6d-8dbc-994005dc01a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = torch.linspace(beta_1, beta_T, T).double()\n",
    "alphas = 1.0 - betas\n",
    "alphas_bar = torch.cumprod(alphas, dim=0)\n",
    "coeff1 = torch.sqrt(1.0 / alphas)\n",
    "coeff2 = coeff1 * (1.0 - alphas) / torch.sqrt(1.0 - alphas_bar)\n",
    "var = betas * (1.0 - F.pad(alphas_bar, [1, 0], value=1)[:T]) / (1.0 - alphas_bar)\n",
    "sqrt_alphas_bar = torch.sqrt(alphas_bar)\n",
    "sqrt_one_minus_alphas_bar = torch.sqrt(1.0 - alphas_bar)\n",
    "used_var = torch.cat([var[1:2], betas[1:]])\n",
    "used_var1 = var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5287aa6c-2a4a-477e-b80a-bb87063f0b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(v, t, x_shape):\n",
    "    device = t.device\n",
    "    out = torch.gather(v, index=t, dim=0).float().to(device)\n",
    "    return out.view([t.shape[0]] + [1] * (len(x_shape) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3147bd-8484-4c45-80dd-2abdd287dd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(epsilon, estimated_epsilon):\n",
    "    return torch.sum((epsilon - estimated_epsilon)**2)/len(epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45f8a76-97b3-43b8-bca7-db197b2718e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def sample(x_T, model):\n",
    "    x_t = x_T\n",
    "    device = x_t.device\n",
    "    for time_step in reversed(range(T)):\n",
    "        if(time_step % 100 == 0):\n",
    "            print(time_step)\n",
    "        t = x_t.new_ones([x_T.shape[0], ], dtype=torch.long) * time_step\n",
    "        mean = extract(coeff1.to(device), t, x_t.shape) * x_t - extract(coeff2.to(device), t, x_t.shape) * model(x_t, t)\n",
    "        var = extract(used_var1.to(device), t, x_t.shape)\n",
    "        # no noise when t == 0\n",
    "        if time_step > 0:\n",
    "            noise = torch.randn_like(x_t)\n",
    "        else:\n",
    "            noise = 0\n",
    "        x_t = mean + torch.sqrt(var) * noise\n",
    "        assert torch.isnan(x_t).int().sum() == 0, \"nan in tensor.\"\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    return x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c57cb0-eaf1-4bbb-bd99-26978e172c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(device, model, optimizer):\n",
    "    \n",
    "    begin_time = time.time()\n",
    "    # train\n",
    "    with open('./loss1.txt', 'w') as file:\n",
    "        for i in range(n_epochs):\n",
    "            for batch_idx, (x, label) in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                x = x.to(device)\n",
    "                epsilon = torch.randn_like(x).to(device)\n",
    "                t = torch.randint(T, (x.shape[0], )).to(device)\n",
    "                x_t = extract(sqrt_alphas_bar.to(device), t, x.shape) * x + extract(sqrt_one_minus_alphas_bar.to(device), t, x.shape) * epsilon\n",
    "                estimated_epsilon = model(x_t, t)\n",
    "                loss_0 = loss(epsilon, estimated_epsilon)\n",
    "                loss_0.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                each_epoch = 1\n",
    "                n_samples = 100\n",
    "                #train\n",
    "                indices = torch.randperm(len(train_dataset))[:n_samples]\n",
    "                x = torch.stack([train_dataset[i][0] for i in indices]).to(device)\n",
    "                t = torch.randint(T, (x.shape[0], )).to(device)\n",
    "                epsilon = torch.randn_like(x).to(device)\n",
    "                x_t = extract(sqrt_alphas_bar.to(device), t, x.shape) * x + extract(sqrt_one_minus_alphas_bar.to(device), t, x.shape) * epsilon\n",
    "                estimated_epsilon = model(x_t, t)\n",
    "                loss_0 = loss(epsilon, estimated_epsilon)\n",
    "                if(i % each_epoch == 0):\n",
    "                    print(\"epoch: \", i, \", training loss: \", loss_0.item())\n",
    "                file.write(str(loss_0.item()) + ' ')\n",
    "                #test\n",
    "                indices = torch.randperm(len(test_dataset))[:n_samples]\n",
    "                x = torch.stack([test_dataset[i][0] for i in indices]).to(device)\n",
    "                t = torch.randint(T, (x.shape[0], )).to(device)\n",
    "                epsilon = torch.randn_like(x).to(device)\n",
    "                x_t = extract(sqrt_alphas_bar.to(device), t, x.shape) * x + extract(sqrt_one_minus_alphas_bar.to(device), t, x.shape) * epsilon\n",
    "                estimated_epsilon = model(x_t, t)\n",
    "                loss_0 = loss(epsilon, estimated_epsilon)\n",
    "                if(i % each_epoch == 0):\n",
    "                    print(\"epoch: \", i, \", testing loss: \", loss_0.item())\n",
    "                file.write(str(loss_0.item()) + '\\n')\n",
    "\n",
    "            if(i % each_epoch == 0):\n",
    "                training_time = time.time() - begin_time\n",
    "                minute = int(training_time // 60)\n",
    "                second = int(training_time % 60)\n",
    "                print(f'time loss {minute}:{second}')\n",
    "            \n",
    "            if(i % 10 == 0 or i == n_epochs-1):\n",
    "                torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),}, \n",
    "                './model_and_optimizer.pth'\n",
    "                )\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f681aab-ea83-4138-a508-2f9cbec07e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    \n",
    "    '''\n",
    "    #start\n",
    "    model = UNet(T=1000, ch=128, ch_mult=[1, 2, 3, 4], attn=[2], num_res_blocks=2, dropout=0.15).to(device)\n",
    "    optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "    '''\n",
    "    \n",
    "    #round 1: 200 200min\n",
    "    #round 2: 200 200min\n",
    "    #round 3: 100 100min\n",
    "    #round 4: 500 500min\n",
    "    #round 5: 500 18hr\n",
    "    #round 6: 500 18hr\n",
    "    #round 7: 500 18hr\n",
    "    \n",
    "    #keep training\n",
    "    model = UNet(T=1000, ch=128, ch_mult=[1, 2, 3, 4], attn=[2], num_res_blocks=2, dropout=0.15).to(device)\n",
    "    optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "    checkpoint = torch.load('./model_and_optimizer.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    \n",
    "    train(device, model, optimizer)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a79689-68f1-47f2-8085-970755fa66e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reload\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(T=1000, ch=128, ch_mult=[1, 2, 3, 4], attn=[2], num_res_blocks=2, dropout=0.15).to(device)\n",
    "checkpoint = torch.load('./model_and_optimizer.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75300f81-f292-4f21-911d-1abd9cf02f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generation\n",
    "x = torch.randn((49, 3, 32, 32)).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    x_0 = sample(x, model)\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "resized_image = torchvision.transforms.Resize((50, 50))(x_0*0.5 + 0.5)\n",
    "save_image(resized_image, './pictures/genera1.png', nrow=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a11f0e6-89c7-4e1d-a294-5d4f7ee5b04e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
