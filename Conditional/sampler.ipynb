{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c16e20-9643-4128-a855-7ad9857c0c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a1b0bb-1099-435c-bc7b-ebae7e95c77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "import math\n",
    "from torch.nn import init\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, T, d_model, dim):\n",
    "        assert d_model % 2 == 0\n",
    "        super().__init__()\n",
    "        emb = torch.arange(0, d_model, step=2) / d_model * math.log(10000)\n",
    "        emb = torch.exp(-emb)\n",
    "        pos = torch.arange(T).float()\n",
    "        emb = pos[:, None] * emb[None, :]\n",
    "        assert list(emb.shape) == [T, d_model // 2]\n",
    "        emb = torch.stack([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
    "        assert list(emb.shape) == [T, d_model // 2, 2]\n",
    "        emb = emb.view(T, d_model)\n",
    "\n",
    "        self.timembedding = nn.Sequential(\n",
    "            nn.Embedding.from_pretrained(emb, freeze=False),\n",
    "            nn.Linear(d_model, dim),\n",
    "            Swish(),\n",
    "            nn.Linear(dim, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        emb = self.timembedding(t)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class ConditionalEmbedding(nn.Module):\n",
    "    def __init__(self, num_labels, d_model, dim):\n",
    "        assert d_model % 2 == 0\n",
    "        super().__init__()\n",
    "        self.condEmbedding = nn.Sequential(\n",
    "            nn.Embedding(num_embeddings=num_labels + 1, embedding_dim=d_model, padding_idx=0),\n",
    "            nn.Linear(d_model, dim),\n",
    "            Swish(),\n",
    "            nn.Linear(dim, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        emb = self.condEmbedding(t)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super().__init__()\n",
    "        self.c1 = nn.Conv2d(in_ch, in_ch, 3, stride=2, padding=1)\n",
    "        self.c2 = nn.Conv2d(in_ch, in_ch, 5, stride=2, padding=2)\n",
    "\n",
    "    def forward(self, x, temb, cemb):\n",
    "        x = self.c1(x) + self.c2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super().__init__()\n",
    "        self.c = nn.Conv2d(in_ch, in_ch, 3, stride=1, padding=1)\n",
    "        self.t = nn.ConvTranspose2d(in_ch, in_ch, 5, 2, 2, 1)\n",
    "\n",
    "    def forward(self, x, temb, cemb):\n",
    "        _, _, H, W = x.shape\n",
    "        x = self.t(x)\n",
    "        x = self.c(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttnBlock(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super().__init__()\n",
    "        self.group_norm = nn.GroupNorm(32, in_ch)\n",
    "        self.proj_q = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
    "        self.proj_k = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
    "        self.proj_v = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
    "        self.proj = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        h = self.group_norm(x)\n",
    "        q = self.proj_q(h)\n",
    "        k = self.proj_k(h)\n",
    "        v = self.proj_v(h)\n",
    "\n",
    "        q = q.permute(0, 2, 3, 1).view(B, H * W, C)\n",
    "        k = k.view(B, C, H * W)\n",
    "        w = torch.bmm(q, k) * (int(C) ** (-0.5))\n",
    "        assert list(w.shape) == [B, H * W, H * W]\n",
    "        w = F.softmax(w, dim=-1)\n",
    "\n",
    "        v = v.permute(0, 2, 3, 1).view(B, H * W, C)\n",
    "        h = torch.bmm(w, v)\n",
    "        assert list(h.shape) == [B, H * W, C]\n",
    "        h = h.view(B, H, W, C).permute(0, 3, 1, 2)\n",
    "        h = self.proj(h)\n",
    "\n",
    "        return x + h\n",
    "\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, tdim, dropout, attn=True):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.GroupNorm(32, in_ch),\n",
    "            Swish(),\n",
    "            nn.Conv2d(in_ch, out_ch, 3, stride=1, padding=1),\n",
    "        )\n",
    "        self.temb_proj = nn.Sequential(\n",
    "            Swish(),\n",
    "            nn.Linear(tdim, out_ch),\n",
    "        )\n",
    "        self.cond_proj = nn.Sequential(\n",
    "            Swish(),\n",
    "            nn.Linear(tdim, out_ch),\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.GroupNorm(32, out_ch),\n",
    "            Swish(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1),\n",
    "        )\n",
    "        if in_ch != out_ch:\n",
    "            self.shortcut = nn.Conv2d(in_ch, out_ch, 1, stride=1, padding=0)\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "        if attn:\n",
    "            self.attn = AttnBlock(out_ch)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "\n",
    "\n",
    "    def forward(self, x, temb, labels):\n",
    "        h = self.block1(x)\n",
    "        h += self.temb_proj(temb)[:, :, None, None]\n",
    "        h += self.cond_proj(labels)[:, :, None, None]\n",
    "        h = self.block2(h)\n",
    "\n",
    "        h = h + self.shortcut(x)\n",
    "        h = self.attn(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, T, num_labels, ch, ch_mult, num_res_blocks, dropout):\n",
    "        super().__init__()\n",
    "        tdim = ch * 4\n",
    "        self.time_embedding = TimeEmbedding(T, ch, tdim)\n",
    "        self.cond_embedding = ConditionalEmbedding(num_labels, ch, tdim)\n",
    "        self.head = nn.Conv2d(3, ch, kernel_size=3, stride=1, padding=1)\n",
    "        self.downblocks = nn.ModuleList()\n",
    "        chs = [ch]  # record output channel when dowmsample for upsample\n",
    "        now_ch = ch\n",
    "        for i, mult in enumerate(ch_mult):\n",
    "            out_ch = ch * mult\n",
    "            for _ in range(num_res_blocks):\n",
    "                self.downblocks.append(ResBlock(in_ch=now_ch, out_ch=out_ch, tdim=tdim, dropout=dropout))\n",
    "                now_ch = out_ch\n",
    "                chs.append(now_ch)\n",
    "            if i != len(ch_mult) - 1:\n",
    "                self.downblocks.append(DownSample(now_ch))\n",
    "                chs.append(now_ch)\n",
    "\n",
    "        self.middleblocks = nn.ModuleList([\n",
    "            ResBlock(now_ch, now_ch, tdim, dropout, attn=True),\n",
    "            ResBlock(now_ch, now_ch, tdim, dropout, attn=False),\n",
    "        ])\n",
    "\n",
    "        self.upblocks = nn.ModuleList()\n",
    "        for i, mult in reversed(list(enumerate(ch_mult))):\n",
    "            out_ch = ch * mult\n",
    "            for _ in range(num_res_blocks + 1):\n",
    "                self.upblocks.append(ResBlock(in_ch=chs.pop() + now_ch, out_ch=out_ch, tdim=tdim, dropout=dropout, attn=False))\n",
    "                now_ch = out_ch\n",
    "            if i != 0:\n",
    "                self.upblocks.append(UpSample(now_ch))\n",
    "        assert len(chs) == 0\n",
    "\n",
    "        self.tail = nn.Sequential(\n",
    "            nn.GroupNorm(32, now_ch),\n",
    "            Swish(),\n",
    "            nn.Conv2d(now_ch, 3, 3, stride=1, padding=1)\n",
    "        )\n",
    " \n",
    "\n",
    "    def forward(self, x, t, labels):\n",
    "        # Timestep embedding\n",
    "        temb = self.time_embedding(t)\n",
    "        cemb = self.cond_embedding(labels)\n",
    "        # Downsampling\n",
    "        h = self.head(x)\n",
    "        hs = [h]\n",
    "        for layer in self.downblocks:\n",
    "            h = layer(h, temb, cemb)\n",
    "            hs.append(h)\n",
    "        # Middle\n",
    "        for layer in self.middleblocks:\n",
    "            h = layer(h, temb, cemb)\n",
    "        # Upsampling\n",
    "        for layer in self.upblocks:\n",
    "            if isinstance(layer, ResBlock):\n",
    "                h = torch.cat([h, hs.pop()], dim=1)\n",
    "            h = layer(h, temb, cemb)\n",
    "        h = self.tail(h)\n",
    "\n",
    "        assert len(hs) == 0\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bb6830-67c1-4b9f-bd1c-5cfbdf035403",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1000\n",
    "beta_1 = 1e-4\n",
    "beta_T = 0.028\n",
    "n_epochs = 100\n",
    "learning_rate = 1e-3\n",
    "w = 1.8\n",
    "p_condi = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242eea29-c78a-4633-9770-73f61693b2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = torch.linspace(beta_1, beta_T, T).double()\n",
    "alphas = 1.0 - betas\n",
    "alphas_bar = torch.cumprod(alphas, dim=0)\n",
    "coeff1 = torch.sqrt(1.0 / alphas)\n",
    "coeff2 = coeff1 * (1.0 - alphas) / torch.sqrt(1.0 - alphas_bar)\n",
    "var = betas * (1.0 - F.pad(alphas_bar, [1, 0], value=1)[:T]) / (1.0 - alphas_bar)\n",
    "sqrt_alphas_bar = torch.sqrt(alphas_bar)\n",
    "sqrt_one_minus_alphas_bar = torch.sqrt(1.0 - alphas_bar)\n",
    "used_var = torch.cat([var[1:2], betas[1:]])\n",
    "used_var1 = var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6576d247-b7fc-40da-9b67-9fcf6fa79c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(v, t, x_shape):\n",
    "    device = t.device\n",
    "    out = torch.gather(v, index=t, dim=0).float().to(device)\n",
    "    return out.view([t.shape[0]] + [1] * (len(x_shape) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38150833-9a52-44a6-858e-1745c12c5a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def DDIM_sample(x_T, model, label, w, eta, simple_var, ddim_step):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        x_t = x_T\n",
    "        device = x_t.device\n",
    "        label0 = torch.zeros_like(label).to(label.device)\n",
    "        ts = torch.linspace(T, 0, (ddim_step + 1)).to(device).to(torch.long)\n",
    "        for i in range(1, ddim_step + 1):\n",
    "            cur_t = ts[i - 1]\n",
    "            prev_t = ts[i]\n",
    "            alphas_bar_cur = alphas_bar[cur_t - 1]\n",
    "            alphas_bar_prev = alphas_bar[prev_t - 1] if prev_t >= 1 else 1\n",
    "\n",
    "            t = x_t.new_ones([x_T.shape[0], ], dtype=torch.long) * (cur_t - 1)\n",
    "            epsilon = (1 + w) * model(x_t, t, label) - w * model(x_t, t, label0)\n",
    "            var = eta * (1-alphas_bar_prev) / (1-alphas_bar_cur) * (1 - alphas_bar_cur/alphas_bar_prev)\n",
    "            x_t = (alphas_bar_prev/alphas_bar_cur)**0.5 * x_t + ((1-alphas_bar_prev-var)**0.5 - (alphas_bar_prev*(1-alphas_bar_cur)/alphas_bar_cur)**0.5) * epsilon + torch.randn_like(x_t) * (var if simple_var == False else (1 - alphas_bar_cur/alphas_bar_prev))**0.5\n",
    "            #gc.collect()\n",
    "            #torch.cuda.empty_cache()\n",
    "        return x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137334b4-3688-4bbc-8d06-3a4f86ee80e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def sample(x_T, model, label, w):\n",
    "    x_t = x_T\n",
    "    device = x_t.device\n",
    "    label0 = torch.zeros_like(label).to(label.device)\n",
    "    for time_step in reversed(range(T)):\n",
    "        if(time_step % 100 == 0):\n",
    "            print(time_step)\n",
    "        t = x_t.new_ones([x_T.shape[0], ], dtype=torch.long) * time_step\n",
    "        epsilon = (1 + w) * model(x_t, t, label) - w * model(x_t, t, label0)\n",
    "        mean = extract(coeff1.to(device), t, x_t.shape) * x_t - extract(coeff2.to(device), t, x_t.shape) * epsilon\n",
    "        var = extract(used_var1.to(device), t, x_t.shape)\n",
    "        # no noise when t == 0\n",
    "        if time_step > 0:\n",
    "            noise = torch.randn_like(x_t)\n",
    "        else:\n",
    "            noise = 0\n",
    "        x_t = mean + torch.sqrt(var) * noise\n",
    "        assert torch.isnan(x_t).int().sum() == 0, \"nan in tensor.\"\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    return x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56094833-285f-47f6-8465-d97913944a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reload\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(T=1000, num_labels=10, ch=128, ch_mult=[1, 2, 2, 2], num_res_blocks=2, dropout=0.15).to(device)\n",
    "checkpoint = torch.load('./model_and_optimizer.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1897a1-bf7c-4c62-b16a-16c29b559502",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random guidance-free Generation\n",
    "num0 = 10\n",
    "x = torch.randn((num0*10, 3, 32, 32)).to(device)\n",
    "\n",
    "label = torch.tensor([i+1 for i in range(10) for _ in range(num0) ])\n",
    "label = label.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    x_0 = sample(x, model, label, w)\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "resized_image = torchvision.transforms.Resize((50, 50))(x_0*0.5 + 0.5)\n",
    "save_image(resized_image, './pictures/genera.png', nrow=num0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde311fc-0909-4677-ac3a-5ee5552127f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conditional Generation\n",
    "num0 = 8\n",
    "x = torch.randn((num0*10, 3, 32, 32)).to(device)\n",
    "\n",
    "label = torch.tensor([i+1 for i in range(10) for _ in range(num0) ])\n",
    "label = label.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    x_0 = sample(x, model, label, 0)\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "resized_image = torchvision.transforms.Resize((50, 50))(x_0*0.5 + 0.5)\n",
    "save_image(resized_image, './pictures/condi_genera.png', nrow=num0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7aa3cc-d904-48cf-830d-d2dbbe07b26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# denoising process\n",
    "x_T = torch.randn((10, 3, 32, 32)).to(device)\n",
    "label = torch.tensor([i+1 for i in range(10)]).to(device)\n",
    "show_list = [900, 300, 200, 100, 90, 80, 70, 60, 50, 40, 30, 20, 10, 0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    \n",
    "    x_t = x_T\n",
    "    device = x_t.device\n",
    "    label0 = torch.zeros_like(label).to(label.device)\n",
    "    X = x_t\n",
    "    for time_step in reversed(range(T)):\n",
    "\n",
    "        t = x_t.new_ones([x_T.shape[0], ], dtype=torch.long) * time_step\n",
    "        epsilon = (1 + w) * model(x_t, t, label) - w * model(x_t, t, label0)\n",
    "        mean = extract(coeff1.to(device), t, x_t.shape) * x_t - extract(coeff2.to(device), t, x_t.shape) * epsilon\n",
    "        var = extract(used_var1.to(device), t, x_t.shape)\n",
    "        # no noise when t == 0\n",
    "        if time_step > 0:\n",
    "            noise = torch.randn_like(x_t)\n",
    "        else:\n",
    "            noise = 0\n",
    "        x_t = mean + torch.sqrt(var) * noise\n",
    "\n",
    "        if(time_step in show_list):\n",
    "            print(time_step)\n",
    "            X = torch.cat((X, x_t), dim=0)\n",
    "\n",
    "        assert torch.isnan(x_t).int().sum() == 0, \"nan in tensor.\"\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "for j in range(10):\n",
    "    for i in range(len(show_list)+1):\n",
    "        if(i == 0 and j ==0):\n",
    "            Y = X[0][None, :]\n",
    "        else:\n",
    "            Y = torch.cat((Y, X[i*10 + j][None, :]), dim=0)\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "resized_image = torchvision.transforms.Resize((50, 50))(Y*0.5 + 0.5)\n",
    "save_image(resized_image, './pictures/denoising.png', nrow=len(show_list)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b9aa3c-b1e6-415c-bb03-5ff02adf23dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare different variance in DDIM\n",
    "label = torch.tensor([1, 2, 3, 8, 9]).to(device)\n",
    "x = torch.randn((5, 3, 32, 32)).to(device)\n",
    "time_steps = 1000\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for eta in [0, 0.2, 0.5, 1.0]:\n",
    "        x_0 = DDIM_sample(x, model, label, w, eta, False, time_steps)\n",
    "        if(eta == 0):\n",
    "            X = x_0\n",
    "        else:\n",
    "            X = torch.cat((X, x_0), dim=0)\n",
    "\n",
    "    x_0 = DDIM_sample(x, model, label, w, 1, True, time_steps)\n",
    "    X = torch.cat((X, x_0), dim=0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "grid = torchvision.utils.make_grid(X*0.5 + 0.5, nrow=5).to(\"cpu\")\n",
    "np_grid = grid.permute(1, 2, 0).numpy()\n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "ax.imshow(np_grid)\n",
    "ax.axis('off')\n",
    "rows = 5\n",
    "cols = 5\n",
    "text_list = ['0', '0.2', '0.5', '1', r'$\\hat{\\sigma}$']\n",
    "for i in range(rows):\n",
    "    ax.text(-23, 35 * i + 17, text_list[i], verticalalignment='center', fontsize=12, color='black')\n",
    "plt.savefig('./pictures/var.png')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8fe394-be83-4bce-964c-499fbb33b06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random guidance-free Generation\n",
    "num0 = 8\n",
    "x = torch.randn((num0*10, 3, 32, 32)).to(device)\n",
    "\n",
    "label = torch.tensor([i+1 for i in range(10) for _ in range(num0) ])\n",
    "label = label.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d135ae1b-71ea-4045-af16-e2a733467710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# different number of steps in DIMM\n",
    "import time\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    x_0 = DDIM_sample(x, model, label, w, 0, False, 100)\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "resized_image = torchvision.transforms.Resize((50, 50))(x_0*0.5 + 0.5)\n",
    "save_image(resized_image, './pictures/ddim_genera.png', nrow=num0)\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b737e2-28a6-47c9-8e64-3316d58b2477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# effect of x_T\n",
    "num0 = 8\n",
    "diff = 0.001\n",
    "\n",
    "for k in range(10):\n",
    "    \n",
    "    original_x = torch.randn((1, 3, 32, 32)).to(device)\n",
    "    x = original_x\n",
    "\n",
    "    for i in range(num0):\n",
    "        x0 = original_x[0]\n",
    "        x0 = x[0] - diff*num0/2 + i*diff\n",
    "        x = torch.cat((x, x0[None, :]), 0)\n",
    "\n",
    "    x = x[1:]\n",
    "\n",
    "    label = torch.tensor([k+1]*num0)\n",
    "    label = label.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # True means use simple variance beta_t in sampling, eta is 1\n",
    "        x0 = DDIM_sample(x, model, label, w, 0, False, 10)\n",
    "    \n",
    "    if(k == 0):\n",
    "        x_0 = x0\n",
    "    else:\n",
    "        x_0 = torch.cat((x_0, x0), 0)\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "resized_image = torchvision.transforms.Resize((50, 50))(x_0*0.5 + 0.5)\n",
    "save_image(resized_image, './pictures/effect.png', nrow=num0)\n",
    "\n",
    "#not very sensitive to x_T, can try inverse function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94be61ed-06ad-4118-8e26-93913f4a8b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "root = '../../data/CIFAR10/'\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "train_dataset = torchvision.datasets.CIFAR10(root=root, train=True, transform=transform, download=False)\n",
    "\n",
    "num_samples = 30000\n",
    "indices = np.random.choice(len(train_dataset), num_samples, replace=False)\n",
    "subset_train_dataset = Subset(train_dataset, indices)\n",
    "\n",
    "print(\"start deleting\")\n",
    "import shutil\n",
    "save_dir = '../samples/original'\n",
    "if os.path.exists(save_dir) and os.path.isdir(save_dir):\n",
    "    shutil.rmtree(save_dir)\n",
    "else:\n",
    "    print(f\"Directory does not exist.\")\n",
    "print(\"done deleting\")\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "print(\"start saving\")\n",
    "def save_images(dataset, save_dir):\n",
    "    for idx, (image, label) in enumerate(dataset):\n",
    "        if(idx%1000 == 0):\n",
    "            print(idx)\n",
    "        image = transforms.ToPILImage()(image)\n",
    "        image.save(os.path.join(save_dir, f'image_{idx}.png'))\n",
    "save_images(subset_train_dataset, save_dir)\n",
    "print(\"done saving\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f855da13-1f1f-484f-bcf0-6d978c01666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IS\n",
    "metrics_dict = torch_fidelity.calculate_metrics(\n",
    "    input1= '../samples/original',\n",
    "    cuda=True,\n",
    "    isc=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f00f418-86cf-4e79-962f-8880648275be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def DDIM_fast_sample(x_T, model, label, eta, simple_var, ddim_step):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        x_t = x_T\n",
    "        device = x_t.device\n",
    "        ts = torch.linspace(T, 0, (ddim_step + 1)).to(device).to(torch.long)\n",
    "        for i in range(1, ddim_step + 1):\n",
    "            cur_t = ts[i - 1]\n",
    "            prev_t = ts[i]\n",
    "            alphas_bar_cur = alphas_bar[cur_t - 1]\n",
    "            alphas_bar_prev = alphas_bar[prev_t - 1] if prev_t >= 1 else 1\n",
    "\n",
    "            t = x_t.new_ones([x_T.shape[0], ], dtype=torch.long) * (cur_t - 1)\n",
    "            epsilon = model(x_t, t, label)\n",
    "            var = eta * (1-alphas_bar_prev) / (1-alphas_bar_cur) * (1 - alphas_bar_cur/alphas_bar_prev)\n",
    "            x_t = (alphas_bar_prev/alphas_bar_cur)**0.5 * x_t + ((1-alphas_bar_prev-var)**0.5 - (alphas_bar_prev*(1-alphas_bar_cur)/alphas_bar_cur)**0.5) * epsilon + torch.randn_like(x_t) * (var if simple_var == False else (1 - alphas_bar_cur/alphas_bar_prev))**0.5\n",
    "        return x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c539643-0c9d-4437-9132-ec6936cd1f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "        \n",
    "    for i in range(100):\n",
    "        print(i)\n",
    "\n",
    "        num_samples = 300\n",
    "        x = torch.randn((num_samples, 3, 32, 32)).to(device)\n",
    "        label = torch.randint(10, (x.shape[0], )) + 1\n",
    "        label = label.to(device)\n",
    "        \n",
    "        x0 = DDIM_fast_sample(x, model, label, 1, False, 1000)\n",
    "        x0 = x0*0.5 +0.5\n",
    "\n",
    "        if(i == 0):\n",
    "            x_0 = x0\n",
    "        else:\n",
    "            x_0 = torch.cat((x_0, x0), 0)\n",
    "        \n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d225941-d06d-4fdb-b992-121a72e68cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "save_dir = '../samples/generated'\n",
    "\n",
    "print(\"start deleting\")\n",
    "import shutil\n",
    "if os.path.exists(save_dir) and os.path.isdir(save_dir):\n",
    "    shutil.rmtree(save_dir)\n",
    "else:\n",
    "    print(f\"Directory does not exist.\")\n",
    "print(\"done deleting\")\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "to_pil = ToPILImage()\n",
    "\n",
    "print(\"start saving\")\n",
    "for i in range(x_0.size(0)):\n",
    "    if(i % 10000 == 0):\n",
    "        print(i)\n",
    "    image_tensor = x_0[i]\n",
    "    image = to_pil(image_tensor)\n",
    "    save_path = os.path.join(save_dir, f'image_{i}.png')\n",
    "    image.save(save_path)\n",
    "print(\"done saving\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0765ea8-5c71-4f6c-9fd8-42d2334cd9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1516e5-a236-41c1-ad4d-1082c7169ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IS and FID\n",
    "metrics_dict = torch_fidelity.calculate_metrics(\n",
    "    input1= '../samples/generated',\n",
    "    input2= '../samples/original',\n",
    "    cuda=True,\n",
    "    isc=True,\n",
    "    fid=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252f32ba-6f90-4a18-9425-16650abdcc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output.txt\", \"w\") as file:\n",
    "    # Write the output to the file\n",
    "    file.write(str(metrics_dict['inception_score_mean']) + '\\n' + str(metrics_dict['frechet_inception_distance']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
